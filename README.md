# LlamaFactory Multi-Arch Docker Build (ARM64 & AMD64)

[![Build Middleware](https://github.com/kingfs/llamafactory-build/actions/workflows/build-middleware.yml/badge.svg)](https://github.com/kingfs/llamafactory-build/actions/workflows/build-middleware.yml)
[![Build App](https://github.com/kingfs/llamafactory-build/actions/workflows/build-app.yml/badge.svg)](https://github.com/kingfs/llamafactory-build/actions/workflows/build-app.yml)
![Docker Pulls](https://img.shields.io/docker/pulls/kingfs/llamafactory)

## ğŸ“– ç®€ä»‹ (Introduction)

æœ¬é¡¹ç›®æ—¨åœ¨ä¸º **NVIDIA ARM64 æ¶æ„è®¾å¤‡** (å¦‚ DGX GH200, Grace Hopper, Jetson Orin ç­‰) æä¾›å¼€ç®±å³ç”¨çš„ [LlamaFactory](https://github.com/hiyouga/LlamaFactory) Docker é•œåƒã€‚

å®˜æ–¹ LlamaFactory é•œåƒç›®å‰ä¸»è¦æ”¯æŒ AMD64 (x86_64)ã€‚åœ¨ ARM64 å¹³å°ä¸Šï¼Œç”±äº `flash-attention`ã€`bitsandbytes` å’Œ `deepspeed` ç­‰æ ¸å¿ƒåº“ç¼ºä¹é¢„ç¼–è¯‘çš„ Wheel åŒ…ï¼Œç”¨æˆ·å¾€å¾€éœ€è¦èŠ±è´¹æ•°å°æ—¶è¿›è¡Œæœ¬åœ°ç¼–è¯‘ï¼Œä¸”ææ˜“é‡åˆ°ç¯å¢ƒä¾èµ–é—®é¢˜ã€‚

æœ¬é¡¹ç›®åˆ©ç”¨ GitHub Actions å®ç° **å…¨è‡ªåŠ¨ã€é€æ˜åŒ–** çš„å¤šæ¶æ„æ„å»ºï¼ˆAMD64 + ARM64ï¼‰ï¼Œå¹¶æ¨é€åˆ° Docker Hub ä¾›ç¤¾åŒºä½¿ç”¨ã€‚

## ğŸ—ï¸ æ„å»ºæ¶æ„ (Architecture)

ä¸ºäº†åº”å¯¹ ARM64 ç¼–è¯‘è€—æ—¶è¿‡é•¿çš„é—®é¢˜ï¼Œæœ¬é¡¹ç›®é‡‡ç”¨ **åˆ†å±‚æ„å»ºç­–ç•¥ (Multi-Stage Build)**ï¼š

1.  **Middleware Image (ä¸­é—´ä»¶é•œåƒ)**: `kingfs/llamafactory:middleware`
    *   **æ›´æ–°é¢‘ç‡**: ä½ (ä»…åœ¨ CUDA æˆ– æ ¸å¿ƒç®—å­åº“ç‰ˆæœ¬æ›´æ–°æ—¶è§¦å‘)
    *   **åŸºç¡€**: `nvcr.io/nvidia/pytorch:25.06-py3` (CUDA 12.4 compatible)
    *   **åŒ…å«**: é¢„ç¼–è¯‘å¥½çš„ FlashAttention-2, DeepSpeed, Bitsandbytes ç­‰é‡å‹ä¾èµ–ã€‚
    *   **ç›®çš„**: ä½œä¸ºåŸºç¡€è®¾æ–½ï¼Œé¿å…æ¯æ¬¡ä»£ç æ›´æ–°éƒ½é‡æ–°ç¼–è¯‘ CUDA ç®—å­ã€‚

2.  **Application Image (åº”ç”¨é•œåƒ)**: `kingfs/llamafactory:latest`
    *   **æ›´æ–°é¢‘ç‡**: é«˜ (è·Ÿéš LlamaFactory å®˜æ–¹ä»£ç æ›´æ–°)
    *   **åŸºç¡€**: `kingfs/llamafactory:middleware`
    *   **åŒ…å«**: LlamaFactory æºä»£ç , HuggingFace ç›¸å…³ Python ä¾èµ–ã€‚
    *   **ç›®çš„**: æä¾›å³æ‹‰å³ç”¨çš„æœ€ç»ˆç”¨æˆ·ç¯å¢ƒã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹ (Quick Start)

### 1. æ‹‰å–é•œåƒ

æ”¯æŒè‡ªåŠ¨è¯†åˆ«æ¶æ„ï¼ˆæ— éœ€æŒ‡å®š tagï¼ŒDocker ä¼šè‡ªåŠ¨æ‹‰å– arm64 æˆ– amd64 ç‰ˆæœ¬ï¼‰ï¼š

```bash
docker pull kingfs/llamafactory:latest
```

### 2. å¯åŠ¨å®¹å™¨ (ä»¥å•æœºå¾®è°ƒä¸ºä¾‹)

```bash
docker run --gpus all \
    --shm-size 16G \
    -v /path/to/your/data:/app/data \
    -v /path/to/your/output:/app/output \
    -it kingfs/llamafactory:latest \
    bash
```

### 3. éªŒè¯ç¯å¢ƒ

è¿›å…¥å®¹å™¨åï¼ŒéªŒè¯æ ¸å¿ƒåº“æ˜¯å¦å®‰è£…æˆåŠŸï¼š

```bash
python -c "import torch; print('Torch:', torch.__version__, torch.cuda.is_available()); \
           import flash_attn; print('FlashAttn:', flash_attn.__version__); \
           import bitsandbytes; print('BnB:', bitsandbytes.__version__)"
```

## ğŸ› ï¸ æ„å»ºç»†èŠ‚ (Build Details)

### ä¾èµ–ç‰ˆæœ¬çŸ©é˜µ

| ç»„ä»¶ | ç‰ˆæœ¬ (Target) | å¤‡æ³¨ |
| :--- | :--- | :--- |
| **Base Image** | `nvcr.io/nvidia/pytorch:25.06-py3` | åŒ…å« CUDA 12.4 å·¥å…·é“¾ |
| **PyTorch** | 2.4.0+ |éš NGC é•œåƒç‰ˆæœ¬ |
| **FlashAttention** | v2.8.3 | æºç ç¼–è¯‘å®‰è£… |
| **DeepSpeed** | Latest | é¢„ç¼–è¯‘ Ops |
| **Bitsandbytes** | Latest | æºç ç¼–è¯‘/è·¨å¹³å° Wheel |

### æœ¬åœ°è‡ªè¡Œæ„å»º (å¯é€‰)

å¦‚æœä½ éœ€è¦åœ¨æœ¬åœ° DGX è®¾å¤‡ä¸Šè‡ªè¡Œæ„å»ºï¼Œå¯ä»¥å‚è€ƒä»¥ä¸‹å‘½ä»¤ï¼š

```bash
# 1. æ„å»ºä¸­é—´ä»¶å±‚ (è€—æ—¶è¾ƒé•¿)
docker build -t local/llamafactory:middleware -f docker/Dockerfile.middleware .

# 2. æ„å»ºåº”ç”¨å±‚ (è€—æ—¶çŸ­)
docker build -t local/llamafactory:latest \
    --build-arg BASE_IMAGE=local/llamafactory:middleware \
    -f docker/Dockerfile.app .
```

## ğŸ“ è´¡çŒ®ä¸æ”¯æŒ

*   æœ¬é•œåƒéå®˜æ–¹é•œåƒï¼Œä¸»è¦ä¸ºäº†è§£å†³ ARM64 ç¤¾åŒºç—›ç‚¹ã€‚
*   æ ¸å¿ƒä»£ç ç‰ˆæƒå½’ [LlamaFactory](https://github.com/hiyouga/LlamaFactory) å›¢é˜Ÿæ‰€æœ‰ã€‚
*   åŸºç¡€é•œåƒç‰ˆæƒå½’ NVIDIA Corporation æ‰€æœ‰ã€‚

å¦‚æœ‰æ„å»ºé—®é¢˜ï¼Œè¯·æäº¤ Issueã€‚
