# docker/Dockerfile.middleware
ARG BASE_IMAGE=nvcr.io/nvidia/pytorch:25.06-py3
FROM ${BASE_IMAGE}

# === 1. 环境准备 (集成 uv + 解除 PEP668) ===
COPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/uv
ENV UV_SYSTEM_PYTHON=1
# 关键：允许 pip/uv 向系统路径安装包
RUN rm -f /usr/lib/python3.*/EXTERNALLY-MANAGED

# === 2. 定义版本和架构 ===
ARG FLASH_ATTN_VERSION=v2.8.3
ARG DEEPSPEED_VERSION=master

# 默认架构列表 (CI 中会根据 amd64/arm64 传入不同的值覆盖这里)
# 8.0=A100, 8.6=3090, 8.9=4090, 9.0=H100/GB200
ARG TORCH_CUDA_ARCH_LIST="8.0;8.6;8.9;9.0"
ENV TORCH_CUDA_ARCH_LIST=${TORCH_CUDA_ARCH_LIST}

# === 3. 编译参数优化 ===
# 原生 Runner 性能较好，MAX_JOBS 可以给到 4
ENV MAX_JOBS=4
ENV FLASH_ATTENTION_FORCE_BUILD=TRUE
# 开启 DeepSpeed 预编译 (Build Ops)
ENV DS_BUILD_OPS=1
ENV DS_BUILD_SPARSE_ATTN=0
ENV DS_BUILD_CPU_ADAM=1

WORKDIR /tmp

# === 4. 安装基础库 ===
RUN apt-get update && apt-get install -y git ninja-build libaio-dev && rm -rf /var/lib/apt/lists/*

# === 5. 编译 Flash-Attention 2 ===
RUN git clone https://github.com/Dao-AILab/flash-attention.git && \
    cd flash-attention && \
    git checkout ${FLASH_ATTN_VERSION} && \
    uv pip install . --no-build-isolation && \
    cd .. && \
    rm -rf flash-attention && \
    uv cache clean

# === 6. 编译 DeepSpeed ===
RUN git clone https://github.com/microsoft/DeepSpeed.git && \
    cd DeepSpeed && \
    git checkout ${DEEPSPEED_VERSION} && \
    # 使用 uv 安装，依赖解析更快
    uv pip install . && \
    cd .. && \
    rm -rf DeepSpeed && \
    uv cache clean

# === 7. 安装 Bitsandbytes ===
RUN uv pip install bitsandbytes --prefer-binary && uv cache clean

# 验证
RUN python -c "import torch; print('Arch List:', torch.cuda.get_arch_list()); import flash_attn; print('FlashAttn:', flash_attn.__version__)"